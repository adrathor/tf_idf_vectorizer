{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg2ooa4DxLiz"
   },
   "source": [
    "## Task-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is implementing TFIDF vectorizer on a collection of text documents. I will compare the results of my own implementation of TFIDF vectorizer with that of sklearns implemenation TFIDF vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnV82tg1xLi0"
   },
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUsYm9wjxLi1"
   },
   "outputs": [],
   "source": [
    "# Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLwmFZfKxLi4"
   },
   "source": [
    "### SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Np4dfQOkxLi4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7Om8YpYxLi6",
    "outputId": "0a3bd0f5-4424-4400-944f-4482a80bd799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTKplK96xLi-",
    "outputId": "53722fa2-6756-4aa0-f179-37b578bb6890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CTiWHygxLjA",
    "outputId": "8d5a9cde-2c29-4afe-f7b4-1547e88dba4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bDKEpbA-xLjD",
    "outputId": "87dafd65-5313-443f-8c6e-1b05cc8c2543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# Here the output is a sparse matrix\n",
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QWo34hexLjF",
    "outputId": "cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfIwx5LzxLjI"
   },
   "source": [
    "### My custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjuCcJwXxLjJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMPUTE IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compidf(uniq,d, data):\n",
    "    c=0\n",
    "    lstc=[]\n",
    "    for u in uniq:    \n",
    "        for i in d:\n",
    "            for j in i.keys():\n",
    "                if u== j:\n",
    "                    c+=1 #count of a word in the whole corpus\n",
    "        lstc.append(c) # list of count of a word in the whole corpus\n",
    "        c=0\n",
    "    wordcount=dict(zip(uniq,lstc)) # dict of word and their count\n",
    "    idf_val=[]\n",
    "    for i, j in wordcount.items():\n",
    "        idf= 1+ math.log((1+len(data))/(1+j)) #formula for IDF\n",
    "        idf_val.append(idf)\n",
    "    idfdict= dict(zip(uniq,idf_val))\n",
    "    return idf_val, idfdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "    d=[]\n",
    "    unique_words=set()\n",
    "    if isinstance(data,list):\n",
    "        for i in data:\n",
    "            d.append(dict(Counter(i.split()))) # d has dicts with words in the data as keys and freq as their values\n",
    "            for j in i.split(\" \"):\n",
    "                if len(j)<2:\n",
    "                    continue\n",
    "                unique_words.add(j)\n",
    "        unique_words= sorted(list(unique_words)) # it is a set of all the unique words\n",
    "        vocab= {j:i for i,j in enumerate(unique_words)} # it is a dict with keys as words and values as indexes\n",
    "        idf_val, idfdict= compidf(unique_words,d, data)\n",
    "        return unique_words, idf_val, idfdict, vocab,d\n",
    "    else:\n",
    "        print(\"wrong datatype\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming corpus to data\n",
    "data = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq, idf_val, idfdict, vocab,d= fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for Computing TF (this function is called in transform function)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comptf(d):\n",
    "    l=[]\n",
    "    listi=[]\n",
    "    alpha=[]\n",
    "    alphai=[]\n",
    "    for k in range(len(d)):    \n",
    "        for i,j in d[k].items():\n",
    "            tf= j/sum(d[k].values()) #formula for term frequency\n",
    "            l.append(tf) #appending tf values\n",
    "            listi.append(i) # appending words\n",
    "        alpha.append(l) #appending list of tf values\n",
    "        alphai.append(listi) #appending list of words\n",
    "        l=[]\n",
    "        listi=[]\n",
    "    return alphai, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,vocab,d):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(dataset, (list,)):\n",
    "        a,b=comptf(d)\n",
    "        tflist=d.copy()\n",
    "        for idx, val in enumerate(b):\n",
    "            zipper= list(zip(a[idx], val))\n",
    "            for i in zipper:\n",
    "                tflist[idx][i[0]]=i[1]\n",
    "        for idx,row in enumerate(tflist):# for each dict in the tflist\n",
    "            for word, tfval in row.items():\n",
    "            # it will return a dict type object where key is the word and values is its TF Value, {word:TF Value}                \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                # we will check if its there in the vocabulary that we build in fit() function\n",
    "                # dict.get() function will return the values, if the key doesn't exits it will return -1\n",
    "                col_index = vocab.get(word, -1) # retreving the dimension number of a word\n",
    "                # if the word exists\n",
    "                if col_index !=-1:\n",
    "                    # we are storing the index of the document\n",
    "                    rows.append(idx)\n",
    "                    # we are storing the dimensions of the word\n",
    "                    columns.append(col_index)\n",
    "                    # we are storing the Mult of TF and IDF of the word\n",
    "                    tfidf= tfval*idfdict[word]\n",
    "                    values.append(tfidf)\n",
    "        return normalize(csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab))))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yup= transform(data,vocab,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(yup[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMxBmVZExLjK"
   },
   "source": [
    "## Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing max features functionality: I will modify my fit and transform functions so that my vocab will contain only 50 terms with top idf scores. Here i will make use of a pickle file to load the corpus from this file and use it as input to your tfidf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHxPLlwNxLjL",
    "outputId": "9abd8e08-0e24-4975-9a13-4d3636d60323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_DJnnR3xLjR"
   },
   "source": [
    "**COMPUTE IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compidf(uniq,d,data):\n",
    "    c=0\n",
    "    lstc=[]\n",
    "    for u in uniq:    \n",
    "        for i in d:\n",
    "            for j in i.keys():\n",
    "                if u== j:\n",
    "                    c+=1 #count of a word in the whole corpus\n",
    "        lstc.append(c) # list of count of a word in the whole corpus\n",
    "        c=0\n",
    "    wordcount=dict(zip(uniq,lstc)) # dict of word and their count\n",
    "    idf_val=[]\n",
    "    for i, j in wordcount.items():\n",
    "        idf= 1+ math.log((1+len(data))/(1+j)) #formula for IDF\n",
    "        idf_val.append(idf)\n",
    "    idfdict= dict(zip(uniq,idf_val))\n",
    "    idfordered=sorted(idfdict.items(), key=lambda x: x[1], reverse=True)\n",
    "    idfordered= idfordered[:50]\n",
    "    idfordered= dict(idfordered)\n",
    "    \n",
    "    return idf_val, idfordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "    d=[]\n",
    "    unique_words=set()\n",
    "    if isinstance(data,list):\n",
    "        for i in data:\n",
    "            d.append(dict(Counter(i.split()))) # d has dicts with words in the data as keys and freq as their values\n",
    "            for j in i.split(\" \"):\n",
    "                if len(j)<2:\n",
    "                    continue\n",
    "                unique_words.add(j)\n",
    "        unique_words= sorted(list(unique_words)) # it is a set of all the unique words\n",
    "        vocab= {j:i for i,j in enumerate(unique_words)} # it is a dict with keys as words and values as indexes\n",
    "        idf_val, idfordered= compidf(unique_words,d, data)\n",
    "        return unique_words, idf_val, idfordered,vocab,d\n",
    "    else:\n",
    "        print(\"wrong datatype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words, idf_val, idfordered,vocab,d= fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function for Computing TF (this function is called in transform function)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comptf(d):\n",
    "    l=[]\n",
    "    listi=[]\n",
    "    alpha=[]\n",
    "    alphai=[]\n",
    "    for k in range(len(d)):    \n",
    "        for i,j in d[k].items():\n",
    "            tf= j/sum(d[k].values()) #formula for term frequency\n",
    "            l.append(tf) #appending tf values\n",
    "            listi.append(i) # appending words\n",
    "        alpha.append(l) #appending list of tf values\n",
    "        alphai.append(listi) #appending list of words\n",
    "        l=[]\n",
    "        listi=[]\n",
    "    return alphai, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,idfordered,d):\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    if isinstance(dataset, (list,)):\n",
    "        a,b=comptf(d)\n",
    "        tflist=d.copy()\n",
    "        for idx, val in enumerate(b):\n",
    "            zipper= list(zip(a[idx], val))\n",
    "            for i in zipper:\n",
    "                tflist[idx][i[0]]=i[1]\n",
    "        for idx,row in enumerate(tflist):# for each dict in the tflist\n",
    "            for word, tfval in row.items():\n",
    "            # it will return a dict type object where key is the word and values is its TF Value, {word:TF Value}                \n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                # we will check if its there in idfordered keys that we build in fit() function\n",
    "                # dict.get() function will return the values, if the key doesn't exits it will return -1\n",
    "                col_index = idfordered.get(word, -1) # retreving the dimension number of a word\n",
    "                # if the word exists\n",
    "                if col_index !=-1:\n",
    "                    # we are storing the index of the document\n",
    "                    rows.append(idx)\n",
    "                    # we are storing the dimensions of the word\n",
    "                    columns.append(col_index)\n",
    "                    # we are storing the Mult of TF and IDF of the word\n",
    "                    tfidf= tfval*idfordered[word]\n",
    "                    values.append(tfidf)\n",
    "        return normalize(csr_matrix((values, (rows,columns)), shape=(len(dataset),len(idfordered))))\n",
    "    else:\n",
    "        print(\"you need to pass list of strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spar= transform(corpus,idfordered,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(spar[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3_Instructions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
